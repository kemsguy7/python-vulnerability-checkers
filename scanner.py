#!/usr/bin/env python
# This script scans a webpage for vulnerabilities

import requests
import re
from urllib.parse import urlparse, urljoin  # Corrected the import statement

class Scanner:
    def __init__(self, url):
        self.target_url = url
        self.target_links = []

    def extract_links_from(self, url):
        response = requests.get(url)  # Corrected variable name 'response'
        return re.findall('(?:href=")(.*?)"', response.content.decode(errors="ignore"))  # Corrected 'target_url' to 'url'

    def crawl(self, url=None):  # url as a default value
        if url is None:
            url = self.target_url
        href_links = self.extract_links_from(url)
        for link in href_links:
            link = urljoin(url, link)  # Corrected 'urlparse.urljoin' to 'urljoin'

            if "#" in link:
                link = link.split("#")[0]

            if self.target_url in link and link not in self.target_links:  # Removes any external link from the website and prints unique links
                self.target_links.append(link)
                print(link)
                self.crawl(link)

# Provide the target URL when creating an instance of the Scanner class
target_url = "http://192.168.63.131"  # Replace with the URL you want to scan
scanner = Scanner(target_url)
scanner.crawl()

